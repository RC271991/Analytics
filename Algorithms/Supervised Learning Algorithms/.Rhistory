results = predict(lda.fit,NewData)
ans_1 = paste('Library Results:',length(which(as.numeric(results) == y[m]))/length(y[m])*100, '%',sep='')
ans_2 = paste(':: Developed Algorithm:',length(which(pred$`Prediction results` == y[m]))/length(y[m])*100, '%',sep='')
paste(ans_1,ans_2)
#For reference: https://www.youtube.com/watch?v=OaIIr29Pj2g
## Quadratic Discriminant Anlaysis
library(matrixcalc)#used for matrix inverse
library(plotly)
#Date Clean-Up
df = iris
df$Species = as.integer(df$Species) #turning factors into integers
#Assigning X matrix and labels
X = cbind(matrix(df[,1]), matrix(df[,2]))
y = df[,5]
QDA = function(X,y) {
#######
##Phi##
#######
phi = list()
for (i in 1:length(unique(y))){
phi[[i]] = length(y[y==i])/length(y)
}
#################
#Mean: Variables#
#################
u = matrix(0,nrow = length(unique(y)), ncol = NCOL(X))
for (i in 1:length(unique(y))){
for (j in 1:NCOL(X)){
u[i,j]  = sum(X[c(which(y == i)),j])/NROW(X[c(which(y == i)),j])
}
}
#####################
#Sigma (Co-Variance)#
#####################
#In QDA: We arn't assuming that each class as similar distribution. Hence, we are taking a seperate Cov-variance for each class
Sigma = list()
X_new = X
for (i in 1:length(unique(y))){
Sigma[[i]] =  cov(X_new[c(which(y == i)),])
}
return(list("phi" = phi, "u" = u, "Sigma" = Sigma))
}
values = QDA(X,y) #calling function to get phi, u, and Sigma
###########################
#Prediction and Boundaries#
###########################
QDA_pred = function(u,Sigma,phi,x){
count = 1
label = array()
for (i in 1:NROW(x)){ #iterating through each data point
val_hold = array()
for (j in 1:NROW(u)){ #iterating per amount of classes (Does not need to be values$u)
val_hold[[j]] = -1/2*log(det(Sigma[[j]])) - 1/2*t(x[i,]-u[j,])%*%matrix.inverse(Sigma[[j]])%*%(z[i,]-u[j,]) + log(phi[[j]])
}
label[[count]] = which(val_hold==max(val_hold)) #Taking argmax and assigning class per data point
count = count + 1
}
return(cbind(x,label)) #combining data set with labels from previous for loop
}
#Setting grid for plot
#Note: This will change per data. So, plot data ahead of time to get x and y boundaries.
x2 <- seq(4,8,0.02)
y2 <- seq(1.8,4.5,0.02)
z <- as.matrix(expand.grid(x2,y2),0) #creating matrix to run QDA_pred function to acquire contour plot of each class;however, if we just want to predict onto test set just set z as test set
z = QDA_pred(values$u,values$Sigma,values$phi,z)
##########
#Plotting#
##########
#Grabbing each class for plot
#Note: Will need to change based on amount of classes per data set
class0 = X[which(y == 1),]
class1 = X[which(y == 2),]
class2 = X[which(y == 3),]
plot_ly() %>%
add_trace(x = z[,1],y = z[,2],z = z[,3], type = 'contour', colorscale = 'Portland',name = 'Class Boundary', showlegend = F)%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: 1', marker = list(color = '55B0EA')) %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 2', marker = list(color = '#239B56'))%>%
add_trace(x = class2[,1],y = class2[,2], type = 'scatter', mode = 'markers',name = 'Class: 3', marker = list(color = '#E6B0AA'))%>%
layout(title = '<b>Iris Data: QDA</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'),
legend = list(orientation = 'h', y = -0.2, x = 0)) %>%
hide_colorbar()
library(plotly)
set.seed(1234)
library(plotly)
##
#Creating 3 class Gaussian distributed data
x1 = rmvnorm(n=100, mean=c(1,1))
x2 = rmvnorm(n=100, mean=c(5,9))
x3 = rmvnorm(n=100, mean=c(10,-1))
X = rbind(x1,x2,x3)
#Initial Plot
plot_ly()%>%
add_trace(x = x1[,1], y = x1[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x2[,1], y = x2[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x3[,1], y = x3[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
layout(title = 'Expectation Maximization')
Amount_classes = 3 #Amount of Classes has to be greater or equal to 2 since the algorithm uses the co-variance
#######################
#Initialize Parameters#
#######################
u = matrix(0,nrow = Amount_classes, ncol = NCOL(X))
for (i in 1:Amount_classes){
randn = runif(2, min(X), max(X)) #selecting 2 random numbers
for (j in 1:NCOL(X)){
u[i,j]  = randn[j]
}
}
#Note: Choose random variance for all classes. It doesn't need to be the Identity matrix
Sigma = list()
for (i in 1:Amount_classes){
Sigma[[i]] = diag(2)
}
phi = c(rep(0.5,Amount_classes)) #we are pegging initial posterior probabilites to 0.5; otherwise, at times we can see a distribution take all the probability from each data point
w = matrix(0, nrow=NROW(X),ncol = Amount_classes)
ll = list()
#E-Step
# calculate w_j^{(i)}
for (i in 1:NROW(X)){
RowTotal = 0
for (j in 1:Amount_classes){
multnormal = phi[[j]]*(1/((2*pi)^(length(u[j,])/2)*sqrt(det(Sigma[[j]])))*exp( -1/2 * t(matrix((X[i,] - u[j,]))) %*% matrix.inverse(Sigma[[j]]) %*% matrix(X[i,] - u[j,]) ))
w[i, j] = multnormal
RowTotal = RowTotal + multnormal
}
w[i,] = w[i,]/as.numeric(RowTotal) #Probability belonging to what class
}
for (j in 1:Amount_classes){
const = sum(w[, j])
phi[[j]] = 1/NROW(X) * const
mu_j = matrix(0,ncol = Amount_classes)
}
library(mvtnorm)
library(mvtnorm)
set.seed(1234)
library(plotly)
##
#Creating 3 class Gaussian distributed data
x1 = rmvnorm(n=100, mean=c(1,1))
x2 = rmvnorm(n=100, mean=c(5,9))
x3 = rmvnorm(n=100, mean=c(10,-1))
X = rbind(x1,x2,x3)
#Initial Plot
plot_ly()%>%
add_trace(x = x1[,1], y = x1[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x2[,1], y = x2[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x3[,1], y = x3[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
layout(title = 'Expectation Maximization')
Amount_classes = 3 #Amount of Classes has to be greater or equal to 2 since the algorithm uses the co-variance
#######################
#Initialize Parameters#
#######################
u = matrix(0,nrow = Amount_classes, ncol = NCOL(X))
for (i in 1:Amount_classes){
randn = runif(2, min(X), max(X)) #selecting 2 random numbers
for (j in 1:NCOL(X)){
u[i,j]  = randn[j]
}
}
#Note: Choose random variance for all classes. It doesn't need to be the Identity matrix
Sigma = list()
for (i in 1:Amount_classes){
Sigma[[i]] = diag(2)
}
phi = c(rep(0.5,Amount_classes)) #we are pegging initial posterior probabilites to 0.5; otherwise, at times we can see a distribution take all the probability from each data point
w = matrix(0, nrow=NROW(X),ncol = Amount_classes)
ll = list()
#E-Step
# calculate w_j^{(i)}
for (i in 1:NROW(X)){
RowTotal = 0
for (j in 1:Amount_classes){
multnormal = phi[[j]]*(1/((2*pi)^(length(u[j,])/2)*sqrt(det(Sigma[[j]])))*exp( -1/2 * t(matrix((X[i,] - u[j,]))) %*% matrix.inverse(Sigma[[j]]) %*% matrix(X[i,] - u[j,]) ))
w[i, j] = multnormal
RowTotal = RowTotal + multnormal
}
w[i,] = w[i,]/as.numeric(RowTotal) #Probability belonging to what class
}
for (j in 1:Amount_classes){
const = sum(w[, j])
phi[[j]] = 1/NROW(X) * const
mu_j = matrix(0,ncol = Amount_classes)
}
phi
mu_j
for (j in 1:Amount_classes){
const = sum(w[, j])
phi[[j]] = 1/NROW(X) * const
mu_j = matrix(0,ncol = Amount_classes)#np.zeros(self.n)
sigma_j = matrix(0, ncol = NCOL(X), nrow = NCOL(X))#np.zeros((self.n, self.n))
for (i in 1:1){
mu_j = mu_j + (X[i,]*w[i,j])
}
}
for (j in 1:Amount_classes){
const = sum(w[, j])
phi[[j]] = 1/NROW(X) * const
mu_j = matrix(0,ncol = Amount_classes)#np.zeros(self.n)
sigma_j = matrix(0, ncol = NCOL(X), nrow = NCOL(X))#np.zeros((self.n, self.n))
for (i in 1:1){
mu_j = mu_j + (X[i,]%*%w[i,j])
}
}
mu_j
X[i,]
X[1,]
w
w[1,1]
X[1,]*w[1,1]
X[1,]
X[1,]*w[1,1]
mu_j+X[1,]*w[1,1]
sigm_j
sigma_j
mu_j[1]
mu_j = matrix(0,ncol = Amount_classes)#np.zeros(self.n)
sigma_j = matrix(0, ncol = NCOL(X), nrow = NCOL(X))#np.zeros((self.n, self.n))
for (j in 1:Amount_classes){
const = sum(w[,j])
phi[[j]] = 1/NROW(X) * const
for (i in 1:1){
mu_j = mu_j[j] + (X[i,]%*%w[i,j])
}
}
mu_j = matrix(0,ncol = Amount_classes)#np.zeros(self.n)
sigma_j = matrix(0, ncol = NCOL(X), nrow = NCOL(X))#np.zeros((self.n, self.n))
for (j in 1:Amount_classes){
const = sum(w[,j])
phi[[j]] = 1/NROW(X) * const
for (i in 1:1){
mu_j = mu_j[j] + (X[i,]*w[i,j])
}
}
mu_j
mu_j = matrix(0,ncol = Amount_classes)#np.zeros(self.n)
sigma_j = matrix(0, ncol = NCOL(X), nrow = NCOL(X))#np.zeros((self.n, self.n))
for (j in 1:Amount_classes){
const = sum(w[,j])
phi[[j]] = 1/NROW(X) * const
for (i in 1:1){
mu_j[j] = mu_j[j] + (X[i,]*w[i,j])
}
}
mu_j = matrix(0,ncol = Amount_classes)#np.zeros(self.n)
sigma_j = matrix(0, ncol = NCOL(X), nrow = NCOL(X))#np.zeros((self.n, self.n))
for (j in 1:Amount_classes){
const = sum(w[,j])
mu_j_vals = 0
sigma_j_vals = 0
phi[[j]] = 1/NROW(X) * const
for (i in 1:1){
mu_j_vals = mu_j_vals + (X[i,]*w[i,j])
}
mu_j[j] = mu_j_vals
}
(X[i,]*w[i,j])
mu_j = matrix(0,ncol = NCOL(X))#np.zeros(self.n)
sigma_j = matrix(0, ncol = NCOL(X), nrow = NCOL(X))#np.zeros((self.n, self.n))
for (j in 1:Amount_classes){
const = sum(w[,j])
mu_j_vals = 0
sigma_j_vals = 0
phi[[j]] = 1/NROW(X) * const
for (i in 1:1){
mu_j_vals = mu_j_vals + (X[i,]*w[i,j])
}
mu_j[j] = mu_j_vals
}
mu_j = matrix(0,ncol = NCOL(X))#np.zeros(self.n)
sigma_j = matrix(0, ncol = NCOL(X), nrow = NCOL(X))#np.zeros((self.n, self.n))
for (j in 1:Amount_classes){
const = sum(w[,j])
mu_j_vals = 0
sigma_j_vals = 0
phi[[j]] = 1/NROW(X) * const
for (i in 1:1){
mu_j_vals = mu_j_vals + (X[i,]*w[i,j])
}
mu_j = mu_j_vals
}
X
u
#M-Step
for (j in 1:Amount_classes){
const = sum(w[,j])
u_sample = matrix(0,ncol = NCOL(X))
Sigma_sample = matrix(0, ncol = NCOL(X), nrow = NCOL(X))
phi[[j]] = 1/NROW(X) * const
for (i in 1:1){
u_sample = u_sample + (X[i,]*w[i,j])
Sigma_sample = w[i,j]*( t( (X[i,] - u[j,]) ) %*% (X[i,] - u[j,]) )
}
u[j,] = u_sample/const
Sigma[[j]] = Sigma_sample/const
}
u[j,]
u
library(mvtnorm)
set.seed(1234)
library(plotly)
##
#Creating 3 class Gaussian distributed data
x1 = rmvnorm(n=100, mean=c(1,1))
x2 = rmvnorm(n=100, mean=c(5,9))
x3 = rmvnorm(n=100, mean=c(10,-1))
X = rbind(x1,x2,x3)
#Initial Plot
plot_ly()%>%
add_trace(x = x1[,1], y = x1[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x2[,1], y = x2[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x3[,1], y = x3[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
layout(title = 'Expectation Maximization')
Amount_classes = 3 #Amount of Classes has to be greater or equal to 2 since the algorithm uses the co-variance
#######################
#Initialize Parameters#
#######################
u = matrix(0,nrow = Amount_classes, ncol = NCOL(X))
for (i in 1:Amount_classes){
randn = runif(2, min(X), max(X)) #selecting 2 random numbers
for (j in 1:NCOL(X)){
u[i,j]  = randn[j]
}
}
#Note: Choose random variance for all classes. It doesn't need to be the Identity matrix
Sigma = list()
for (i in 1:Amount_classes){
Sigma[[i]] = diag(2)
}
phi = c(rep(0.5,Amount_classes)) #we are pegging initial posterior probabilites to 0.5; otherwise, at times we can see a distribution take all the probability from each data point
w = matrix(0, nrow=NROW(X),ncol = Amount_classes)
ll = list()
#E-Step
# calculate w_j^{(i)}
for (i in 1:NROW(X)){
RowTotal = 0
for (j in 1:Amount_classes){
multnormal = phi[[j]]*(1/((2*pi)^(length(u[j,])/2)*sqrt(det(Sigma[[j]])))*exp( -1/2 * t(matrix((X[i,] - u[j,]))) %*% matrix.inverse(Sigma[[j]]) %*% matrix(X[i,] - u[j,]) ))
w[i, j] = multnormal
RowTotal = RowTotal + multnormal
}
w[i,] = w[i,]/as.numeric(RowTotal) #Probability belonging to what class
}
#M-Step
for (j in 1:Amount_classes){
const = sum(w[,j])
u_sample = matrix(0,ncol = NCOL(X))
Sigma_sample = matrix(0, ncol = NCOL(X), nrow = NCOL(X))
phi[[j]] = 1/NROW(X) * const
for (i in 1:NROW(X)){
u_sample = u_sample + (X[i,]*w[i,j])
Sigma_sample = w[i,j]*( t( (X[i,] - u[j,]) ) %*% (X[i,] - u[j,]) )
}
u[j,] = u_sample/const
Sigma[[j]] = Sigma_sample/const
}
Sigma
w[i,j]*( t( (X[i,] - u[j,]) ) %*% (X[i,] - u[j,]) )
sigma
library(mvtnorm)
set.seed(1234)
library(plotly)
##
#Creating 3 class Gaussian distributed data
x1 = rmvnorm(n=100, mean=c(1,1))
x2 = rmvnorm(n=100, mean=c(5,9))
x3 = rmvnorm(n=100, mean=c(10,-1))
X = rbind(x1,x2,x3)
#Initial Plot
plot_ly()%>%
add_trace(x = x1[,1], y = x1[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x2[,1], y = x2[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x3[,1], y = x3[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
layout(title = 'Expectation Maximization')
Amount_classes = 3 #Amount of Classes has to be greater or equal to 2 since the algorithm uses the co-variance
#######################
#Initialize Parameters#
#######################
u = matrix(0,nrow = Amount_classes, ncol = NCOL(X))
for (i in 1:Amount_classes){
randn = runif(2, min(X), max(X)) #selecting 2 random numbers
for (j in 1:NCOL(X)){
u[i,j]  = randn[j]
}
}
#Note: Choose random variance for all classes. It doesn't need to be the Identity matrix
Sigma = list()
for (i in 1:Amount_classes){
Sigma[[i]] = diag(2)
}
phi = c(rep(0.5,Amount_classes)) #we are pegging initial posterior probabilites to 0.5; otherwise, at times we can see a distribution take all the probability from each data point
w = matrix(0, nrow=NROW(X),ncol = Amount_classes)
ll = list()
#E-Step
# calculate w_j^{(i)}
for (i in 1:NROW(X)){
RowTotal = 0
for (j in 1:Amount_classes){
multnormal = phi[[j]]*(1/((2*pi)^(length(u[j,])/2)*sqrt(det(Sigma[[j]])))*exp( -1/2 * t(matrix((X[i,] - u[j,]))) %*% matrix.inverse(Sigma[[j]]) %*% matrix(X[i,] - u[j,]) ))
w[i, j] = multnormal
RowTotal = RowTotal + multnormal
}
w[i,] = w[i,]/as.numeric(RowTotal) #Probability belonging to what class
}
Sigma_sample = matrix(0, ncol = NCOL(X), nrow = NCOL(X))
w[1,1]*( t( (X[1,] - u[1,]) ) %*% (X[1,] - u[1,]) )
w[i,j
]
w[1,1]
t( (X[1,] - u[1,]) )
X[1,]
w[i,j]
t( (X[i,] - u[j,])
t( (X[i,] - u[j,]))
(X[i,] - u[j,])
X[1,]
(X[i,] - u[j,])
u[1,]
t( matrix((X[i,] - u[j,]) )
t( matrix((X[i,] - u[j,]) ) )
matrix((X[i,] - u[j,])
matrix((X[i,] - u[j,]))
t( matrix((X[i,] - u[j,])) ) %*% matrix(X[i,] - u[j,])
matrix((X[i,] - u[j,]))  %*% t(matrix(X[i,] - u[j,]))
w[i,j]*(matrix((X[i,] - u[j,]))  %*% t(matrix(X[i,] - u[j,])) )
Sigma_sample + w[i,j]*(matrix((X[i,] - u[j,]))  %*% t(matrix(X[i,] - u[j,])) )
library(mvtnorm)
set.seed(1234)
library(plotly)
##
#Creating 3 class Gaussian distributed data
x1 = rmvnorm(n=100, mean=c(1,1))
x2 = rmvnorm(n=100, mean=c(5,9))
x3 = rmvnorm(n=100, mean=c(10,-1))
X = rbind(x1,x2,x3)
#Initial Plot
plot_ly()%>%
add_trace(x = x1[,1], y = x1[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x2[,1], y = x2[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
add_trace(x = x3[,1], y = x3[,2], type = 'scatter', mode = 'markers',showlegend = F, marker = list(color = '#5499C7')) %>%
layout(title = 'Expectation Maximization')
Amount_classes = 3 #Amount of Classes has to be greater or equal to 2 since the algorithm uses the co-variance
#######################
#Initialize Parameters#
#######################
u = matrix(0,nrow = Amount_classes, ncol = NCOL(X))
for (i in 1:Amount_classes){
randn = runif(2, min(X), max(X)) #selecting 2 random numbers
for (j in 1:NCOL(X)){
u[i,j]  = randn[j]
}
}
#Note: Choose random variance for all classes. It doesn't need to be the Identity matrix
Sigma = list()
for (i in 1:Amount_classes){
Sigma[[i]] = diag(2)
}
phi = c(rep(0.5,Amount_classes)) #we are pegging initial posterior probabilites to 0.5; otherwise, at times we can see a distribution take all the probability from each data point
w = matrix(0, nrow=NROW(X),ncol = Amount_classes)
ll = list()
#E-Step
# calculate w_j^{(i)}
for (i in 1:NROW(X)){
RowTotal = 0
for (j in 1:Amount_classes){
multnormal = phi[[j]]*(1/((2*pi)^(length(u[j,])/2)*sqrt(det(Sigma[[j]])))*exp( -1/2 * t(matrix((X[i,] - u[j,]))) %*% matrix.inverse(Sigma[[j]]) %*% matrix(X[i,] - u[j,]) ))
w[i, j] = multnormal
RowTotal = RowTotal + multnormal
}
w[i,] = w[i,]/as.numeric(RowTotal) #Probability belonging to what class
}
#M-Step
for (j in 1:Amount_classes){
const = sum(w[,j])
u_sample = matrix(0,ncol = NCOL(X))
Sigma_sample = matrix(0, ncol = NCOL(X), nrow = NCOL(X))
phi[[j]] = 1/NROW(X) * const
for (i in 1:NROW(X)){
u_sample = u_sample + (X[i,]*w[i,j])
Sigma_sample = Sigma_sample + w[i,j]*(matrix((X[i,] - u[j,]))  %*% t(matrix(X[i,] - u[j,])) )
}
u[j,] = u_sample/const
Sigma[[j]] = Sigma_sample/const
}
Sigma[[j]]
Sigma
log_likehood = list()
for (i in 1:NROW(X)){
pxkpk = 0
for (j in 1:1){
pxkpk =  pxkpk + phi[[j]]*(1/((2*pi)^(length(u[j,])/2)*sqrt(det(Sigma[[j]])))*exp( -1/2 * t(matrix((X[i,] - u[j,]))) %*% matrix.inverse(Sigma[[j]]) %*% matrix(X[i,] - u[j,]) ))
}
#log_likehood[[i]] = log(pxkpk)
}
log_likehood = list()
for (i in 1:NROW(X)){
pxkpk = 0
for (j in 1:1){
pxkpk =  pxkpk + phi[[j]]*(1/((2*pi)^(length(u[j,])/2)*sqrt(det(Sigma[[j]])))*exp( -1/2 * t(matrix((X[i,] - u[j,]))) %*% matrix.inverse(Sigma[[j]]) %*% matrix(X[i,] - u[j,]) ))
}
log_likehood[[i]] = log(pxkpk)
}
View(log_likehood)
log_likehood = 0
for (i in 1:NROW(X)){
pxkpk = 0
for (j in 1:Amount_classes){
pxkpk =  pxkpk + phi[[j]]*(1/((2*pi)^(length(u[j,])/2)*sqrt(det(Sigma[[j]])))*exp( -1/2 * t(matrix((X[i,] - u[j,]))) %*% matrix.inverse(Sigma[[j]]) %*% matrix(X[i,] - u[j,]) ))
}
log_likehood = log_likehood + log(pxkpk)
}
View(log_likehood)
