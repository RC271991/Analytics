#y =mx+b
Y1 = slope*X[,1]+intercept
X =cbind(X,y)
class0 = X[which(y == -1),]
class1 = X[which(y == 1),]
#Plotting data and wTx+b respect to SMO algorithm:
plot_ly() %>%
add_trace(x = X[,1], y = Y1, type = 'scatter', mode = 'lines', name = 'wTx+b')%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: -1') %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 1')%>%
layout(title = '<b>Iris Data -- SMO-SVM</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'))
library(plotly)
#reference: https://pdfs.semanticscholar.org/59ee/e096b49d66f39891eb88a6c84cc89acba12d.pdf
#Date Clean-Up
df = iris
df = df[df$Species != 'versicolor',]
df$Species = as.integer(df$Species)
df$Species[df$Species == 1] = -1
df$Species[df$Species == 3] = 1
df = df[df$Sepal.Width != 2.5,]
#Assigning X matrix and labels
X = cbind(matrix(df[,1]), matrix(df[,2]))
Y = df[,5]
K = X %*% t(X) #linear kernel: Note model can be applied with gaussian or polynomial kernel
SMOSVM = function(X,Y,K){
alphas = rep(0, NROW(X))
C = 1.2 #By changing C will effect placement of hyperplane
b = 0
eps = 10^(-5)
maxiter = 1000
count = 0
while (count < maxiter) {
changed_alphas <- 0
for (i in 1:NROW(X)){
y1 = Y[i]
E1 = b + sum(alphas*Y*K[,i]) - y1
if((y1*E1 < -eps & alphas[i] < C) || (y1 >  eps & alphas[i] > 0)) {
#i!=j according to algorithm
j = sample(nrow(X),1)
ans = T
while (ans == T) {
j = sample(nrow(X),1)
if (j != i){
ans = F
}
}
y2 = Y[j]
E2 = b + sum(alphas * Y * K[,j]) - y2
s = y1*y2
#Computing L and H
alpha1 = alphas[i]
alpha2 = alphas[j]
if (y1 != y2) {
L = max(0, alphas[j]-alphas[i])
H = min(C, C + alphas[j]-alphas[i])
}else{
L = max(0, alphas[j]+alphas[i]-C)
H = min(C, alphas[j]+alphas[i])
}
if (L == H) {
next
}
#####
#eta#
#####
kii = K[i,i]
kjj = K[j,j]
kij = K[i,j]
eta = 2 * kij - kii - kjj
if (eta >= 0) {
next
}
########
#Alphas#
########
#Computing new alpha
alphas[j] = alphas[j] - y2 * (E1 - E2)/eta
#Clipping
alphas[j] = min(H, alphas[j])
alphas[j] = max(L, alphas[j])
if (abs(alphas[j]-alpha2) < eps){
alphas[j] = alpha2
next
}
alphas[i] = alphas[i]+s*(alpha2-alphas[j])
######
#Bias#
######
b1 = b - E1-y1*(alphas[i]-alpha1)*kij-y2*(alphas[j]-alpha2)*kij
b2 = b - E2-y1*(alphas[i]-alpha1)*kij-y2*(alphas[j]-alpha2)*kij
if ( alphas[i] > 0 && alphas[i] < C) {
b = b1
}else if (alphas[j] > 0 && alphas[j] < C) {
b = b2
}else {
b = (b1+b2)/2
}
changed_alphas = changed_alphas + 1
}
}
if (changed_alphas == 0) {
count = count + 1
}else {
count =  0
}
}
#solving for w and returning w an b
index = which(alphas > 0)
X1 = X[index,]
Y1 = Y[index]
alphas1 = alphas[index]
w = t(alphas1 * Y1) %*% X1
return(list("w" = w, "b" = b))
}
#Calling function with specified parameters
#Note: What is returned is the w vector and bias b
values = SMOSVM(X,Y,K)
intercept = -values$b/values$w[2]
slope = -values$w[1]/values$w[2]
#y =mx+b
Y1 = slope*X[,1]+intercept
X =cbind(X,y)
class0 = X[which(Y == -1),]
class1 = X[which(Y == 1),]
#Plotting data and wTx+b respect to SMO algorithm:
plot_ly() %>%
add_trace(x = X[,1], y = Y1, type = 'scatter', mode = 'lines', name = 'wTx+b')%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: -1') %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 1')%>%
layout(title = '<b>Iris Data -- SMO-SVM</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'))
#Plotting data and wTx+b respect to SMO algorithm:
plot_ly() %>%
add_trace(x = X[,1], y = Y1, type = 'scatter', mode = 'lines', name = 'wTx+b')%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: -1', marker = list(color = '247CB3')) %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 1')%>%
layout(title = '<b>Iris Data -- SMO-SVM</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'))
library(plotly)
#reference: https://pdfs.semanticscholar.org/59ee/e096b49d66f39891eb88a6c84cc89acba12d.pdf
#Date Clean-Up
df = iris
df = df[df$Species != 'versicolor',]
df$Species = as.integer(df$Species)
df$Species[df$Species == 1] = -1
df$Species[df$Species == 3] = 1
df = df[df$Sepal.Width != 2.5,]
#Assigning X matrix and labels
X = cbind(matrix(df[,1]), matrix(df[,2]))
Y = df[,5]
K = X %*% t(X) #linear kernel: Note model can be applied with gaussian or polynomial kernel
SMOSVM = function(X,Y,K){
alphas = rep(0, NROW(X))
C = 1.2 #By changing C will effect placement of hyperplane
b = 0
eps = 10^(-5)
maxiter = 1000
count = 0
while (count < maxiter) {
changed_alphas <- 0
for (i in 1:NROW(X)){
y1 = Y[i]
E1 = b + sum(alphas*Y*K[,i]) - y1
if((y1*E1 < -eps & alphas[i] < C) || (y1 >  eps & alphas[i] > 0)) {
#i!=j according to algorithm
j = sample(nrow(X),1)
ans = T
while (ans == T) {
j = sample(nrow(X),1)
if (j != i){
ans = F
}
}
y2 = Y[j]
E2 = b + sum(alphas * Y * K[,j]) - y2
s = y1*y2
#Computing L and H
alpha1 = alphas[i]
alpha2 = alphas[j]
if (y1 != y2) {
L = max(0, alphas[j]-alphas[i])
H = min(C, C + alphas[j]-alphas[i])
}else{
L = max(0, alphas[j]+alphas[i]-C)
H = min(C, alphas[j]+alphas[i])
}
if (L == H) {
next
}
#####
#eta#
#####
kii = K[i,i]
kjj = K[j,j]
kij = K[i,j]
eta = 2 * kij - kii - kjj
if (eta >= 0) {
next
}
########
#Alphas#
########
#Computing new alpha
alphas[j] = alphas[j] - y2 * (E1 - E2)/eta
#Clipping
alphas[j] = min(H, alphas[j])
alphas[j] = max(L, alphas[j])
if (abs(alphas[j]-alpha2) < eps){
alphas[j] = alpha2
next
}
alphas[i] = alphas[i]+s*(alpha2-alphas[j])
######
#Bias#
######
b1 = b - E1-y1*(alphas[i]-alpha1)*kij-y2*(alphas[j]-alpha2)*kij
b2 = b - E2-y1*(alphas[i]-alpha1)*kij-y2*(alphas[j]-alpha2)*kij
if ( alphas[i] > 0 && alphas[i] < C) {
b = b1
}else if (alphas[j] > 0 && alphas[j] < C) {
b = b2
}else {
b = (b1+b2)/2
}
changed_alphas = changed_alphas + 1
}
}
if (changed_alphas == 0) {
count = count + 1
}else {
count =  0
}
}
#solving for w and returning w an b
index = which(alphas > 0)
X1 = X[index,]
Y1 = Y[index]
alphas1 = alphas[index]
w = t(alphas1 * Y1) %*% X1
return(list("w" = w, "b" = b))
}
#Calling function with specified parameters
#Note: What is returned is the w vector and bias b
values = SMOSVM(X,Y,K)
intercept = -values$b/values$w[2]
slope = -values$w[1]/values$w[2]
#y =mx+b
Y1 = slope*X[,1]+intercept
X =cbind(X,y)
class0 = X[which(Y == -1),]
class1 = X[which(Y == 1),]
#Plotting data and wTx+b respect to SMO algorithm:
plot_ly() %>%
add_trace(x = X[,1], y = Y1, type = 'scatter', mode = 'lines', name = 'wTx+b')%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: -1', marker = list(color = '55B0EA')) %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 1')%>%
layout(title = '<b>Iris Data -- SMO-SVM</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'))
set.seed(1234)
x1 <- rnorm(100, 1, 2)
x2 <- rnorm(100)
y <- sign(-1 - 2 * x1 + 4 * x2 )
y[ y == -1] <- 0
X = cbind(1, matrix(x1),matrix(x2))
LogisticRegGradientDescent = function(X,y,alpha) {
#Logistic Regression by Gradient Descent
iter = 5000
theta = matrix(rep(0,ncol(X)))
for (i in 1:5000) {
m = length(y)
sigmoid = 1 /(1 + exp(-(X%*%theta)))
# updating the theta
theta = theta -  alpha / m * ( t(X) %*% (sigmoid - y) )
}
beta = theta
#Fitted values based on coefficents: Make sure to change coef to the amount of parameters in model
fitted.values = exp(beta[1] + beta[2]*X[1:m,2] +beta[3]*X[1:m,3])/(1 + exp(beta[1] + beta[2]*X[1:m,2] +beta[3]*X[1:m,3]))
return(list("Coefficients" = beta, "fitted.values" = fitted.values))
}
values = LogisticRegGradientDescent(X,y, 0.1)
#Reference: Understanding Decision boundaries for logistic Regression https://statinfer.com/203-5-2-decision-boundary-logistic-regression/
#If p(y) > 0.5 then class = 1: Y = mx+b ==> X2 = (-b0/b1)*X1 + -b0/b2
intercept = -values$Coefficients[1]/values$Coefficients[3]
slope = -values$Coefficients[2]/values$Coefficients[3]
Y = slope*X[,2]+ intercept
class0 = X[which(y == 0),]
class1 = X[which(y == 1),]
plot_ly() %>%
add_trace(x = X[,2], y = Y, type = 'scatter', mode = 'lines', name = 'Decision Boundary')%>%
add_trace(x = class0[,2],y = class0[,3], type = 'scatter', mode = 'markers',name = 'Class: 0') %>%
add_trace(x = class1[,2],y = class1[,3], type = 'scatter', mode = 'markers',name = 'Class: 1') %>%
layout(title = '<b>Logistic Regression: Gradient Descent</b>')
#For refernce: Variables phi, u0,u1, and Sigma are from http://cs229.stanford.edu/notes/cs229-notes2.pdf
library(matrixcalc)#used for matrix inverse
#Date Clean-Up
df = iris
df = df[df$Species != 'versicolor',]
df$Species = as.integer(df$Species)
df$Species[df$Species == 1] = 0
df$Species[df$Species == 3] = 1
#Assigning X matrix and labels
X = cbind(matrix(df[,1]), matrix(df[,2]))
y = df[,5]
#For refernce: Variables phi, u0,u1, and Sigma are from http://cs229.stanford.edu/notes/cs229-notes2.pdf
library(matrixcalc)#used for matrix inverse
#Date Clean-Up
df = iris
df = df[df$Species != 'versicolor',]
df$Species = as.integer(df$Species)
df$Species[df$Species == 1] = 0
df$Species[df$Species == 3] = 1
#Assigning X matrix and labels
X = cbind(matrix(df[,1]), matrix(df[,3]))
y = df[,5]
#For refernce: Variables phi, u0,u1, and Sigma are from http://cs229.stanford.edu/notes/cs229-notes2.pdf
library(matrixcalc)#used for matrix inverse
#Date Clean-Up
df = iris
df = df[df$Species != 'versicolor',]
df$Species = as.integer(df$Species)
df$Species[df$Species == 1] = 0
df$Species[df$Species == 3] = 1
#Assigning X matrix and labels
X = cbind(matrix(df[,1]), matrix(df[,3]))
y = df[,5]
#######
##Phi##
#######
phi = length(y[y==1])/length(y)
#############
##U0 and #U1#
#############
u0 = list()
u1 = list()
for (i in 1:NCOL(X)){
u0[[i]] = sum(X[c(which(y == 0)),i])/NROW(X[c(which(y == 0)),i])
u1[[i]] = sum(X[c(which(y == 1)),i])/NROW(X[c(which(y == 1)),i])
}
u0 = matrix(unlist(u0))
u1 = matrix(unlist(u1))
#####################
#Sigma (Co-Variance)#
#####################
index = 1
sigma = list()
X_new = X
for (i in 1:NCOL(X_new)){
#Subtracting u0 &u1 from each class per independent variable
X_new[c(which(y == 0)),i]  = X_new[c(which(y == 0)),1] - u0[i,1]
X_new[c(which(y == 1)),i]  = X_new[c(which(y == 1)),i] - u1[i,1]
for (j in 1:NCOL(X_new)){
m = NCOL(X_new)
numerator = sum(X_new[,i]*t(X_new[,j]))
sigma[[index]] = numerator/m
index = index+1
}
}
sigma = unlist(sigma)
sigma = matrix(sigma, nrow = NCOL(X_new), ncol = NCOL(X_new))
sigma
matrix.inverse(sigma)*(u0 - u1)
matrix.inverse(sigma)
matrix.inverse(sigma)
u0
u0-u1
matrix.inverse(sigma)%*%(u0 - u1)
#For refernce: Variables phi, u0,u1, and Sigma are from http://cs229.stanford.edu/notes/cs229-notes2.pdf
library(matrixcalc)#used for matrix inverse
#Date Clean-Up
df = iris
df = df[df$Species != 'versicolor',]
df$Species = as.integer(df$Species)
df$Species[df$Species == 1] = 0
df$Species[df$Species == 3] = 1
#Assigning X matrix and labels
X = cbind(matrix(df[,1]), matrix(df[,2]))
y = df[,5]
phi = length(y[y==1])/length(y)
#############
##U0 and #U1#
#############
u0 = list()
u1 = list()
for (i in 1:NCOL(X)){
u0[[i]] = sum(X[c(which(y == 0)),i])/NROW(X[c(which(y == 0)),i])
u1[[i]] = sum(X[c(which(y == 1)),i])/NROW(X[c(which(y == 1)),i])
}
u0 = matrix(unlist(u0))
u1 = matrix(unlist(u1))
#####################
#Sigma (Co-Variance)#
#####################
index = 1
sigma = list()
X_new = X
for (i in 1:NCOL(X_new)){
#Subtracting u0 &u1 from each class per independent variable
X_new[c(which(y == 0)),i]  = X_new[c(which(y == 0)),1] - u0[i,1]
X_new[c(which(y == 1)),i]  = X_new[c(which(y == 1)),i] - u1[i,1]
for (j in 1:NCOL(X_new)){
m = NCOL(X_new)
numerator = sum(X_new[,i]*t(X_new[,j]))
sigma[[index]] = numerator/m
index = index+1
}
}
sigma = unlist(sigma)
sigma = matrix(sigma, nrow = NCOL(X_new), ncol = NCOL(X_new))
matrix.inverse(sigma)%*%(u0 - u1)
X =cbind(X,y)
class0 = X[which(y == 0),]
class1 = X[which(y == 1),]
plot_ly() %>%
#add_trace(x = X[,1], y = Y1, type = 'scatter', mode = 'lines', name = 'wTx+b')%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: -1') %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 1')%>%
layout(title = '<b>Iris Data --Gaussian</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'))
##
vec = matrix.inverse(sigma)%*%(u0 - u1)
vec
plot_ly() %>%
add_trace(x = c(0,vec[1,]), y = c(0,vec[2,]), type = 'scatter', mode = 'lines', name = 'wTx+b')%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: -1') %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 1')%>%
layout(title = '<b>Iris Data --Gaussian</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'))
vec[1,]
vec[2,]
m = -vec[2,]/vec[1,]
b = m*vec[1,] + vec[2,]
m*vec[1,] + vec[2,]
m*vec[1,]
plot_ly() %>%
add_trace(x = c(0,vec[1,]), y = c(0,vec[2,]), type = 'scatter', mode = 'lines', name = 'wTx+b')#%>%
vec
m
b = vac[2,] - m*vec[1,]
b = vec[2,] - m*vec[1,]
Y = m*1:10+b
plot_ly() %>%
add_trace(x = 1:10, y = Y, type = 'scatter', mode = 'lines', name = 'wTx+b')%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: -1') %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 1')%>%
layout(title = '<b>Iris Data --Gaussian</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'))
library(matrixcalc)#used for matrix inverse
#Date Clean-Up
df = iris
df = df[df$Species != 'versicolor',]
df$Species = as.integer(df$Species)
df$Species[df$Species == 1] = 0
df$Species[df$Species == 3] = 1
#Assigning X matrix and labels
X = cbind(matrix(df[,1]), matrix(df[,2]))
y = df[,5]
#######
##Phi##
#######
phi = length(y[y==1])/length(y)
#############
##U0 and #U1#
#############
u0 = list()
u1 = list()
for (i in 1:NCOL(X)){
u0[[i]] = sum(X[c(which(y == 0)),i])/NROW(X[c(which(y == 0)),i])
u1[[i]] = sum(X[c(which(y == 1)),i])/NROW(X[c(which(y == 1)),i])
}
u0 = matrix(unlist(u0))
u1 = matrix(unlist(u1))
#####################
#Sigma (Co-Variance)#
#####################
index = 1
sigma = list()
X_new = X
for (i in 1:NCOL(X_new)){
#Subtracting u0 &u1 from each class per independent variable
X_new[c(which(y == 0)),i]  = X_new[c(which(y == 0)),1] - u0[i,1]
X_new[c(which(y == 1)),i]  = X_new[c(which(y == 1)),i] - u1[i,1]
for (j in 1:NCOL(X_new)){
m = NCOL(X_new)
numerator = sum(X_new[,i]*t(X_new[,j]))
sigma[[index]] = numerator/m
index = index+1
}
}
sigma = unlist(sigma)
sigma = matrix(sigma, nrow = NCOL(X_new), ncol = NCOL(X_new))
vec = matrix.inverse(sigma)%*%(u0 - u1)
X =cbind(X,y)
class0 = X[which(y == 0),]
class1 = X[which(y == 1),]
m = -vec[2,]/vec[1,]
b = vec[2,] - m*vec[1,]
Y = m*1:10+b
plot_ly() %>%
add_trace(x = 1:10, y = Y, type = 'scatter', mode = 'lines', name = 'wTx+b')%>%
add_trace(x = class0[,1],y = class0[,2], type = 'scatter', mode = 'markers',name = 'Class: -1') %>%
add_trace(x = class1[,1],y = class1[,2], type = 'scatter', mode = 'markers',name = 'Class: 1')%>%
layout(title = '<b>Iris Data --Gaussian</b>',
xaxis = list(title = 'Sepal.Length'),
yaxis = list(title = 'Sepal.Width'))
